{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Introduction\n",
    "\n",
    "\n",
    "\n",
    "## What is and Why Spark\n",
    "\n",
    "Spark is a lightning fast solution to cloud computing by running in memory\n",
    "\n",
    "Speed, powerful caching, real time, deployment, polyglot\n",
    "\n",
    "Spark works best with python, despite developed in Scala\n",
    "\n",
    "\n",
    "## How PySpark work\n",
    "\n",
    "distributed computing\n",
    "\n",
    "pyspark runs 100x fater than hadroop map reduce\n",
    "\n",
    "PySpark works similar to pandas.\n",
    "\n",
    "### Abstractions:\n",
    "RDD - distrubted collection of objects\n",
    "Dataframe - distrubted dataset of tabular data\n",
    "    integrated SQL and ML algorithms\n",
    "    \n",
    "### Immutable\n",
    "changes create new object references\n",
    "old versions are unchanged\n",
    "\n",
    "### lazy\n",
    "computed does not happen until output is requested\n",
    "fuse all operation at the end in one api\n",
    "\n",
    "## PySpark vs Pandas\n",
    "\n",
    "### PySpark\n",
    "works well on huge data sets\n",
    "Native SQL, can easily switch between sql and dataframe\n",
    "math operations rely on internal functions, don't use numpy. This keeps the exection in the JVM without actually running any python code, it's just a transformation and fast\n",
    "\n",
    "PySpark has no indices\n",
    "\n",
    "if you really want to make histograms, use df.sample(False, 0.1).toPandas().hist()\n",
    "\n",
    "\n",
    "#### udf\n",
    "need to be deterministic\n",
    "\n",
    "#### SQL\n",
    "\n",
    "df = spark.sql('select * from foo' )\n",
    "\n",
    "\n",
    "### Pandas \n",
    "Easy plotting\n",
    "Indices\n",
    "works naturally with numpy\n",
    "3rd party sql using sqlite\n",
    "\n",
    "\n",
    "## Best practices\n",
    "\n",
    "1. use built in functions\n",
    "2. unse the same version of python and packages on cluster as driver, ie from the same conda environment\n",
    "3. checkout the UI at http://localhost:4040/\n",
    "4. kearn about SSH port forwarding. run on notebook, jupyter hub\n",
    "5. MLlib, equivalent to scikit-learn\n",
    "6. RTFM: documentation\n",
    "\n",
    "## Bad practices\n",
    "\n",
    "1. iterate through rows\n",
    "2. hard code a master in your driver. -> user spark-submit for that\n",
    "3. df.toPandas.head() is bad, -> df.limit(5).toPandas(), filter first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "1. https://www.youtube.com/watch?v=XrpSRCwISdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
