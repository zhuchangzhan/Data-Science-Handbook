{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP \n",
    "\n",
    "*** How to deal with text data ***\n",
    "\n",
    "## Path\n",
    "\n",
    "### Speech to Text\n",
    "\n",
    "### Text to Speech\n",
    "\n",
    "\n",
    "## NLP Skill Sets\n",
    "\n",
    "### Programing: \n",
    "\n",
    "#### Data Packages:\n",
    "pandas, sklearn, re, \n",
    "\n",
    "#### NLP Packages:\n",
    "nltk, TextBlob, gensim\n",
    "\n",
    "### Math and Stats: \n",
    "\n",
    "#### Data Cleaning:\n",
    "corpus, document-term matrix\n",
    "\n",
    "#### Exploratory Data Analysis:\n",
    "word counts\n",
    "\n",
    "#### NLP Applications\n",
    "1. Sentiment Analysis\n",
    "2. Topic Modelling\n",
    "3. Text Generation\n",
    "\n",
    "### Communication\n",
    "\n",
    "#### Design\n",
    "scope, visualize, extract insights\n",
    "\n",
    "#### Domain expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering: \n",
    "\n",
    "## Packages:\n",
    "Requests, Beautifulshop, Pickle\n",
    "\n",
    "more advanced: TD-IDF: Term Frequency-Inverse Document Frequency,\n",
    "Stemming/Lemmatization\n",
    "\n",
    "\n",
    "## Limit your scope\n",
    "constraining the amount of data you need to gather based on question\n",
    "\n",
    "# Data Cleaning\n",
    "use pandas to create dataframe\n",
    "Format 1: corpus: a collection of texts\n",
    "\n",
    "Format 2: Document-Term Matrix\n",
    "1. Clean Text: remove excess, unnecessary parts of the text\n",
    "2. Tokenize Text: split the text into smaller pieces\n",
    "3. Document-Term Matrix: put into a matrix so a machine can read it\n",
    "\n",
    "Methods:\n",
    "1. Remove Punctuation\n",
    "2. Remove numbers\n",
    "3. make all into lower case\n",
    "\n",
    "# Tokenization\n",
    "split text into smaller pieces. most common token is a word, but can also be a sentence\n",
    "\n",
    "need to filter out words with very little meaning, or **stop word**. These words doesn't help machine to process\n",
    "\n",
    "## Bag of words model\n",
    "simple format that ignores order\n",
    "scikit-learn: count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "make sure the input data is good and not **GIGO**\n",
    "But the purpose is to take initial look at the data and see if results of basic analysis made sense\n",
    "python packages: wordcloud, matplotlib\n",
    "\n",
    "more advanced: seaborn, plotly, bokeh\n",
    "\n",
    "## Data Methods:\n",
    "1. Top words\n",
    "2. Vocabulary\n",
    "3. Profanity\n",
    "4. What would be interesting for the context of NLP?\n",
    "\n",
    "## Aggregate\n",
    "\n",
    "\n",
    "## Visualize\n",
    "1. Bar plot\n",
    "2. Word cloud, see if the words make sense.\n",
    "\n",
    "## Insights\n",
    "\n",
    "1. Does the data make sense?\n",
    "2. Can we further clean the data?\n",
    "3. What are some initial findings?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "python package: TextBlob\n",
    "\n",
    "WordNet (Princeton)\n",
    "\n",
    "order matters\n",
    "\n",
    "more advanced: Logistic Regression, Naive Bayes\n",
    "\n",
    "## How TextBlob Sentiment Analysis Works:\n",
    "English language is mapped out. Done by Tom De Smedt\n",
    "\n",
    "polarity between -1 and 1\n",
    "subjectivity between 0 and 1\n",
    "\n",
    "average over all polarity and subjectivity that was in database\n",
    "\"not\" x polarity -0.5\n",
    "\"very\" x subjectivity 1.3\n",
    "\n",
    "This is a rule-based approach using knowledge-based techniques. Can use statistical models such as naive bayes to train and identify the right words\n",
    "\n",
    "How sentiment change over time would be interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "python package: gensim, by Radim Rehurek.\n",
    "\n",
    "Most popular technique: Latent Dirichlet Allocation (LDA): Hidden probability distributions.\n",
    "\n",
    "nltk used for parts-of-speech tagging\n",
    "\n",
    "More Advanced: Latent Semantic Indexing (LSI), Non-Negative MAtrix Factorization (NMF)\n",
    "\n",
    "## Input: document-term matrix. \n",
    "order doesn't matter\n",
    "\n",
    "## Output: find themes\n",
    "\n",
    "## How LDA Works\n",
    "\n",
    "1. identify how many topics. gensim will find the best distribution of each topic.\n",
    "\n",
    "takes a long time to make it make sense.\n",
    "logging can help tune hyperparameters\n",
    "\n",
    "### popular tricks:\n",
    "1. look at nouns and adjectie only, using pos_tag\" penn treebank project\n",
    "2. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "## simple way:  Markov Chains\n",
    "\n",
    "## more complex way: LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy - good for object oriented. \n",
    "\n",
    "\n",
    "## Reference:\n",
    "1. https://www.youtube.com/watch?v=xvqsFTUsOmc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
